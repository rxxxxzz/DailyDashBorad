[
  {
    "name": "native-sparse-attention-pytorch",
    "full_name": "lucidrains/native-sparse-attention-pytorch",
    "description": "Implementation of the sparse attention pattern proposed by the Deepseek team in their \"Native Sparse Attention\" paper",
    "url": "https://github.com/lucidrains/native-sparse-attention-pytorch",
    "stars": 362,
    "daily_stars": 5,
    "language": "Python",
    "topics": [
      "artificial-intelligence",
      "attention",
      "deep-learning"
    ],
    "created_at": "2025-02-19T03:37:52",
    "updated_at": "2025-02-21T16:21:18"
  },
  {
    "name": "native-sparse-attention-pytorch",
    "full_name": "lucidrains/native-sparse-attention-pytorch",
    "description": "Implementation of the sparse attention pattern proposed by the Deepseek team in their \"Native Sparse Attention\" paper",
    "url": "https://github.com/lucidrains/native-sparse-attention-pytorch",
    "stars": 362,
    "daily_stars": 0,
    "language": "Python",
    "topics": [
      "artificial-intelligence",
      "attention",
      "deep-learning"
    ],
    "created_at": "2025-02-19T03:37:52",
    "updated_at": "2025-02-21T16:21:18"
  },
  {
    "name": "deep-cross-attention",
    "full_name": "lucidrains/deep-cross-attention",
    "description": "Implementation of the proposed DeepCrossAttention by Heddes et al while at Google research, in Pytorch",
    "url": "https://github.com/lucidrains/deep-cross-attention",
    "stars": 71,
    "daily_stars": 1,
    "language": null,
    "topics": [
      "artificial-intelligence",
      "attention-mechanism",
      "deep-learning",
      "residuals",
      "transformers"
    ],
    "created_at": "2025-02-18T15:28:43",
    "updated_at": "2025-02-21T14:36:47"
  },
  {
    "name": "deep-cross-attention",
    "full_name": "lucidrains/deep-cross-attention",
    "description": "Implementation of the proposed DeepCrossAttention by Heddes et al while at Google research, in Pytorch",
    "url": "https://github.com/lucidrains/deep-cross-attention",
    "stars": 71,
    "daily_stars": 0,
    "language": null,
    "topics": [
      "artificial-intelligence",
      "attention-mechanism",
      "deep-learning",
      "residuals",
      "transformers"
    ],
    "created_at": "2025-02-18T15:28:43",
    "updated_at": "2025-02-21T14:36:47"
  },
  {
    "name": "A5-PII-Anonymizer",
    "full_name": "AgenticA5/A5-PII-Anonymizer",
    "description": "Desktop App with Built-In LLM for Removing Personal Identifiable Information in Documents",
    "url": "https://github.com/AgenticA5/A5-PII-Anonymizer",
    "stars": 38,
    "daily_stars": 8,
    "language": "JavaScript",
    "topics": [
      "ai",
      "ai-safety",
      "alm",
      "anonymisation",
      "anonymity",
      "anonymization",
      "artificial-intelligence",
      "desktop-application",
      "electron",
      "electron-desktop",
      "gdpr",
      "hipaa",
      "linux",
      "llm",
      "macos",
      "personal-identifiable-information",
      "pii",
      "privacy",
      "reasoning",
      "windows"
    ],
    "created_at": "2025-02-17T22:10:04",
    "updated_at": "2025-02-21T16:39:23"
  },
  {
    "name": "Furnace",
    "full_name": "fsprojects/Furnace",
    "description": "Production-grade ML - F# power & precision guiding Torch performance",
    "url": "https://github.com/fsprojects/Furnace",
    "stars": 37,
    "daily_stars": 1,
    "language": "F#",
    "topics": [
      "ai",
      "data-science",
      "differential-equations",
      "dotnet",
      "fsharp",
      "llm-framework",
      "machine-learning",
      "ml",
      "optimization"
    ],
    "created_at": "2025-02-13T21:59:00",
    "updated_at": "2025-02-21T13:11:24"
  },
  {
    "name": "transformer-lm-gan",
    "full_name": "lucidrains/transformer-lm-gan",
    "description": "Explorations into adversarial losses on top of autoregressive loss for language modeling",
    "url": "https://github.com/lucidrains/transformer-lm-gan",
    "stars": 31,
    "daily_stars": 0,
    "language": "Python",
    "topics": [
      "adversarial-learning",
      "artificial-intelligence",
      "autoregressive-transformers",
      "deep-learning"
    ],
    "created_at": "2025-02-14T14:56:14",
    "updated_at": "2025-02-19T20:37:16"
  },
  {
    "name": "transformer-lm-gan",
    "full_name": "lucidrains/transformer-lm-gan",
    "description": "Explorations into adversarial losses on top of autoregressive loss for language modeling",
    "url": "https://github.com/lucidrains/transformer-lm-gan",
    "stars": 31,
    "daily_stars": 0,
    "language": "Python",
    "topics": [
      "adversarial-learning",
      "artificial-intelligence",
      "autoregressive-transformers",
      "deep-learning"
    ],
    "created_at": "2025-02-14T14:56:14",
    "updated_at": "2025-02-19T20:37:16"
  },
  {
    "name": "II-OlimpiadaAI",
    "full_name": "OlimpiadaAI/II-OlimpiadaAI",
    "description": "Oficjalne repozytorium drugiej edycji og√≥lnopolskiej Olimpiady Sztucznej Inteligencji",
    "url": "https://github.com/OlimpiadaAI/II-OlimpiadaAI",
    "stars": 26,
    "daily_stars": 0,
    "language": "Jupyter Notebook",
    "topics": [
      "artificial-intelligence",
      "education",
      "high-school"
    ],
    "created_at": "2025-02-16T10:42:42",
    "updated_at": "2025-02-21T08:18:26"
  },
  {
    "name": "BoT",
    "full_name": "zihao-ai/BoT",
    "description": "Beak long thought processes of o1-like LLMs",
    "url": "https://github.com/zihao-ai/BoT",
    "stars": 18,
    "daily_stars": 0,
    "language": "Python",
    "topics": [
      "ai-agents",
      "backdoor-attacks",
      "chain-of-thought",
      "deepseek",
      "deepseek-r1",
      "large-language-models",
      "qwq",
      "reasoning-language-models"
    ],
    "created_at": "2025-02-17T08:09:39",
    "updated_at": "2025-02-21T09:13:12"
  }
]